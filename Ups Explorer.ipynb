{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn JSON lines into JSON array\n",
    "def json_line_to_array(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = map(lambda line: line.rstrip(), lines) #remove \\n\n",
    "        json_str = '[' + ','.join(lines) + ']'\n",
    "        return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_pos = pd.read_json(json_line_to_array('data/politicos/json/balanced_multi.json'))\n",
    "reddit_neg = pd.read_json(json_line_to_array('data/politicos/json/balanced_single.json'))\n",
    "# reddit_pos = pd.read_json('data/politicos/json/multi.json')\n",
    "# reddit_neg = pd.read_json('data/politicos/json/single.json')\n",
    "first_post = pd.concat([reddit_pos, reddit_neg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label / outcome variable\n",
    "multi_post = first_post['total_posts']>1\n",
    "first_post.drop(['total_posts', 'post_ids', 'post_datetimes', 'last_post_datetime'], axis=1, inplace=True)\n",
    "multi_post.index.rename('multi_post', inplace=True)\n",
    "multi_post.name = 'multi_post'\n",
    "\n",
    "# Ups/downs\n",
    "first_post.rename(columns={'first_post_ups':'ups',\n",
    "                           'first_post_downs':'downs'}, inplace=True)\n",
    "first_post['has_ups'] = first_post['ups'].apply(lambda ups: 1 if ups > 0 else 0)\n",
    "#first_post['neg_ups'] = first_post['ups'].apply(lambda ups: 1 if ups < 0 else 0)\n",
    "# bug: sample is missing downs\n",
    "\n",
    "# Responses\n",
    "first_post['responses_avg_word_ct'] = first_post['first_post_responses'].apply(lambda responses: 0 if isinstance(responses, float) else np.sum([len(response.split()) for response in responses]) * 1.0 / len(responses) )\n",
    "first_post['has_long_response'] = first_post['first_post_responses'].apply(lambda responses: 0 if isinstance(responses, float) else (1 if np.max([len(response.split()) for response in responses])>20 else 0))\n",
    "first_post.rename(columns={'first_post_avg_response_ups':'responses_ups_avg',\n",
    "                           'first_post_avg_response_downs':'responses_downs_avg',\n",
    "                           'first_post_total_responses':'responses_total'}, inplace=True)\n",
    "first_post.drop(['first_post_responses','first_post_response_ups', 'first_post_response_downs'], axis=1, inplace=True) #not doing text analysis for now\n",
    "first_post.fillna(0, inplace=True) #response stats are NaN if no responses\n",
    "# responses_ups_avg is actually an interaction term multiplied by has_responses\n",
    "first_post['has_responses'] = (first_post['responses_total']>0).astype(int)\n",
    "\n",
    "# Body\n",
    "first_post['word_count'] = first_post['first_post_body'].apply(lambda post: len(post.split()))\n",
    "first_post['long_post'] = first_post['word_count'].apply(lambda wc: 1 if wc > 20 else 0)\n",
    "\n",
    "# Parent type\n",
    "first_post['is_response'] = first_post['parent_type']=='t1'\n",
    "first_post['is_response'] = first_post['is_response'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare comments that got upvotes vs no upvotes with similar:\n",
    "# long_post\n",
    "# has_responses\n",
    "# has_long_response\n",
    "# is_response\n",
    "# date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_ups           0         1\n",
      "long_post                    \n",
      "0          0.176225  0.823775\n",
      "1          0.179275  0.820725\n",
      "has_ups               0         1\n",
      "has_responses                    \n",
      "0              0.161852  0.838148\n",
      "1              0.203942  0.796058\n",
      "has_ups                   0         1\n",
      "has_long_response                    \n",
      "0                  0.169428  0.830572\n",
      "1                  0.207979  0.792021\n",
      "has_ups             0         1\n",
      "is_response                    \n",
      "0            0.202845  0.797155\n",
      "1            0.148102  0.851898\n"
     ]
    }
   ],
   "source": [
    "# Percentage of people who got upvotes in each category\n",
    "for col in ['long_post','has_responses','has_long_response','is_response']:\n",
    "    print pd.crosstab(first_post[col], first_post['has_ups']).apply(lambda row: row * 1.0 / row.sum(), axis=1)\n",
    "# Counterintuitive: more people got upvotes when they got no response\n",
    "# To investigate: responses but no upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No ups, multi-post\n",
    "multi_post_no_ups = reddit_pos[ reddit_pos['first_post_ups']==0 ]['first_post_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ups, single-post\n",
    "single_post_ups = reddit_neg[ reddit_neg['first_post_ups']>0 ][['first_post_body', 'first_post_ups']]\n",
    "#single_post_ups['first_post_body'] = single_post_ups['first_post_body'].apply(lambda line: line.encode('utf-8'))\n",
    "#single_post_ups.to_csv('single_post_ups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Response, no upvotes\n",
    "first_post['first_post_body'] = first_post['first_post_body'].apply(lambda line: line.encode('utf-8'))\n",
    "#response_no_upvotes = first_post[ np.logical_and(first_post['has_responses']==1, first_post['has_ups']==0) ]['first_post_body']\n",
    "#response_no_upvotes.to_csv('response_no_upvotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82232733164345262"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(first_post['has_ups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37588522748059688"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(first_post['has_responses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
